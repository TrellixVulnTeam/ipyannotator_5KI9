{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "import urllib\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def _download_url(url, root, filename=None):\n",
    "    \"\"\"Download a file from a url and place it in root.\n",
    "    Args:\n",
    "        url (str): URL to download file from\n",
    "        root (str): Directory to place downloaded file in\n",
    "        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n",
    "    \"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "    fpath = os.path.join(root, filename)\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    \n",
    "    if not os.path.isfile(fpath):\n",
    "        try:\n",
    "            print('Downloading ' + url + ' to ' + fpath)\n",
    "            urllib.request.urlretrieve(url, fpath)\n",
    "        except (urllib.error.URLError, IOError) as e:\n",
    "            if url[:5] == 'https':\n",
    "                url = url.replace('https:', 'http:')\n",
    "                print('Failed download. Trying https -> http instead.'\n",
    "                        ' Downloading ' + url + ' to ' + fpath)\n",
    "                urllib.request.urlretrieve(url, fpath)\n",
    "    else:\n",
    "        print(f'File {filename} already exists, skip download.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _extract_tar(tar_path, output_dir):\n",
    "    try:\n",
    "        print('Extracting...')\n",
    "        with tarfile.open(tar_path) as tar:\n",
    "            tar.extractall(output_dir)\n",
    "    except (tarfile.TarError, IOError, zlib.error) as e:\n",
    "        print('Failed to extract!', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dowload datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cifar10(output_dir):\n",
    "    \"\"\"\n",
    "    Download the cifar10 dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    dataset_dir = output_dir / 'cifar10'\n",
    "        \n",
    "    _download_url(url='https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz', root=output_dir)\n",
    "    \n",
    "    if not dataset_dir.is_dir():\n",
    "        _extract_tar(output_dir / 'cifar10.tgz', output_dir)\n",
    "    else:\n",
    "        print(f'Directory {dataset_dir} already exists, skip extraction.')\n",
    "    \n",
    "    print('Generating train/test data..')\n",
    "    imdir_train = dataset_dir / 'train'\n",
    "    imdir_test = dataset_dir / 'test'\n",
    "\n",
    "    # split train/test\n",
    "    train = [Path(p) for p in glob.glob(f'{imdir_train}/*/*')]\n",
    "    test = [Path(p) for p in glob.glob(f'{imdir_test}/*/*')]\n",
    "\n",
    "    # generate data for annotations.json  \n",
    "    # {'image-file.jpg': ['label1.jpg']}\n",
    "    annotations_train = dict((str(p), [f'{p.parts[-2]}.jpg']) for p in train)\n",
    "    annotations_test = dict((str(p), [f'{p.parts[-2]}.jpg']) for p in test)\n",
    "\n",
    "    train_path = dataset_dir / 'annotations_train.json'\n",
    "    test_path = dataset_dir / 'annotations_test.json'\n",
    "    \n",
    "    with open(train_path, 'w') as f:  \n",
    "        json.dump(annotations_train, f)\n",
    "    \n",
    "    with open(test_path, 'w') as f:  \n",
    "        json.dump(annotations_test, f)\n",
    "    print(\"Done\")\n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "cifar_train_p, cifar_test_p = get_cifar10('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_oxford_102_flowers(output_dir):\n",
    "    \"\"\"\n",
    "    Download the oxford flowers dataset.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    dataset_dir = output_dir / 'oxford-102-flowers'\n",
    "    \n",
    "    _download_url(url='https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz', root=output_dir)\n",
    "    \n",
    "    if not dataset_dir.is_dir():\n",
    "        _extract_tar(output_dir / 'oxford-102-flowers.tgz', output_dir)\n",
    "    else:\n",
    "        print(f'Directory {dataset_dir} already exists, skip extraction.')\n",
    "        \n",
    "    print('Generating train/test data..')\n",
    "    with open(dataset_dir / 'train.txt', 'r') as f:\n",
    "        annotations_train = dict(tuple(line.split()) for line in f)\n",
    "\n",
    "    annotations_train = {str(dataset_dir / k): [v+'.jpg'] for k, v in annotations_train.items()} \n",
    "    \n",
    "    with open(dataset_dir / 'test.txt', 'r') as f:\n",
    "        annotations_test = dict(tuple(line.split()) for line in f)\n",
    "\n",
    "    annotations_test = {str(dataset_dir / k): [v+'.jpg'] for k, v in annotations_test.items()} \n",
    "\n",
    "    train_path = dataset_dir / 'annotations_train.json'\n",
    "    test_path = dataset_dir / 'annotations_test.json'\n",
    "    \n",
    "    with open(train_path, 'w') as f:  \n",
    "        json.dump(annotations_train, f)\n",
    "    \n",
    "    with open(test_path, 'w') as f:  \n",
    "        json.dump(annotations_test, f)\n",
    "    print(\"Done\")\n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "flowers102_train_p, flowers102_test_p = get_oxford_102_flowers('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cub_200_2011(output_dir):\n",
    "    \"\"\"\n",
    "    Download the CUB 200 2001 dataset.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    dataset_dir = output_dir / 'CUB_200_2011'\n",
    "    \n",
    "    _download_url(url='https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz', root=output_dir)\n",
    "\n",
    "    if not dataset_dir.is_dir():\n",
    "        _extract_tar(output_dir / 'CUB_200_2011.tgz', output_dir)\n",
    "    else:\n",
    "        print(f'Directory {dataset_dir} already exists, skip extraction.')\n",
    "        \n",
    "    print('Generating train/test data..')\n",
    "    with open(dataset_dir / 'images.txt','r') as f:\n",
    "        image_id_map = dict(tuple(line.split()) for line in f)\n",
    "\n",
    "    with open(dataset_dir / 'classes.txt','r') as f:\n",
    "        class_id_map = dict(tuple(line.split()) for line in f)\n",
    "\n",
    "    with open(dataset_dir / 'train_test_split.txt','r') as f:\n",
    "        splitter = dict(tuple(line.split()) for line in f)\n",
    "\n",
    "    # image ids for test/train\n",
    "    train_k = [k for k, v in splitter.items() if v == '0']\n",
    "    test_k = [k for k, v in splitter.items() if v == '1']\n",
    "\n",
    "    with open(dataset_dir / 'image_class_labels.txt','r') as f:\n",
    "        anno_ = dict(tuple(line.split()) for line in f)\n",
    "\n",
    "    annotations_train = {str(dataset_dir / 'images' / image_id_map[k]): [class_id_map[v]+'.jpg'] for k, v in anno_.items() if k in train_k} \n",
    "    annotations_test = {str(dataset_dir / 'images' / image_id_map[k]): [class_id_map[v]+'.jpg'] for k, v in anno_.items() if k in test_k} \n",
    "\n",
    "    train_path = dataset_dir  / 'annotations_train.json'\n",
    "    test_path = dataset_dir  / 'annotations_test.json'\n",
    "    \n",
    "    with open(train_path, 'w') as f:  \n",
    "        json.dump(annotations_train, f)\n",
    "    \n",
    "    with open(test_path, 'w') as f:  \n",
    "        json.dump(annotations_test, f)\n",
    "    print(\"Done\")\n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "cub200_train_p, cub200_test_p = get_cub_200_2011('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
